üåê EPIC L ‚Äî LLM Pool como Servi√ßo (desacoplado + seguro)
L1 ‚Äî Core Service (API, servidor e config)

Objetivo: Servi√ßo stateless com gRPC e HTTP compat√≠veis, hot-reload de config.

Tarefas

 Contrato gRPC

 proto/vvtv/llmpool/v1/llmpool.proto com Query, Answer, Task, Strategy, EnsembleDecision, Health.

 request_id, tenant_id, project_id, deadline_ms, max_tokens, meta.

 Servidor

 llm-pool/src/server/grpc.rs (tonic) e http.rs (axum/actix) espelhando o proto.

 router.rs: roteia /v1/infer ‚Üí orchestrator::infer().

 config.rs: carregar llm-pool.toml com hot-reload (notify).

 Orquestrador

 orchestrator.rs: valida Query, enriquece deadline, chama ensemble, formata Answer.

 Enforce limites: max_deadline_ms, max_prompt_bytes, max_tokens.

Arquivos

proto/vvtv/llmpool/v1/llmpool.proto

llm-pool/src/server/{grpc.rs,http.rs,router.rs}

llm-pool/src/{config.rs,orchestrator.rs,errors.rs}

Testes

 Round-trip gRPC/HTTP equivalentes.

 Valida√ß√£o de bounds (deadline acima do limite ‚Üí 400/INVALID_ARGUMENT).

 Health check OK.

CI

 Job gera stubs (Rust/Python/TS) e verifica drift do proto.

Docs

 docs/LLMPOOL_API.md (exemplos gRPC/HTTP).

L2 ‚Äî Providers (drivers plug√°veis)

Objetivo: Falar com engines reais: Ollama, vLLM, HTTP gen√©rico.

Tarefas

 Trait Provider

 providers/mod.rs: Provider { name(), supports(task), infer(query) }.

 Ollama

 provider_ollama.rs: POST /api/generate, mapear model, prompt, max_tokens.

 vLLM

 provider_vllm.rs: POST /generate (OpenAI-like).

 HTTP gen√©rico

 provider_http.rs: schema configur√°vel (mapeia campos via template).

 Healthcheck & backoff

 health.rs: ping peri√≥dico, marca provider degraded/unhealthy.

Arquivos

llm-pool/src/providers/{mod.rs,ollama.rs,vllm.rs,http.rs,health.rs}

Testes

 Mocks dos 3 providers.

 Health flapping ‚Üí backoff exponencial.

CI

 Contract tests (respostas sint√©ticas por task).

Docs

 docs/PROVIDERS.md (campos suportados, exemplos de config).

L3 ‚Äî Ensemble & QoS (lat√™ncia, hedge, breaker)

Objetivo: Estrat√©gias FASTEST/VOTING/WEIGHTED/CONSENSUS/JUDGE, hedged requests, circuit breaker.

Tarefas

 Ensemble

 ensemble.rs: implementar 5 estrat√©gias.

 Similaridade (VOTING/CONSENSUS): minhash/levenshtein simples, opcional embed.

 WEIGHTED: pesos do config + win-rate hist√≥rico.

 JUDGE: concatenar candidatos, prompt curto de julgamento, prazo menor; fallback ‚Üí VOTING.

 Hedged requests

 Disparar 2¬™ tentativa (outro provider) ap√≥s hedge_after_ms no FASTEST.

 Circuit Breaker

 Por provider e por task (janela 50, fail rate >10% abre 5min).

 Timeout orchestration

 Dividir deadline_ms entre providers e judge com margem (e.g., 70/30).

Arquivos

llm-pool/src/{ensemble.rs,qos/{hedge.rs,breaker.rs}}

Testes

 Fallback chain: JUDGE‚ÜíVOTING‚ÜíWEIGHTED‚ÜíFASTEST.

 Hedged reduz p95 em cen√°rio com um provider lento.

 Breaker abre/fecha corretamente.

CI

 Tests de lat√™ncia simulada, asserts de p95.

Docs

 docs/ENSEMBLE.md (quando usar cada estrat√©gia; custos/lat√™ncia).

L4 ‚Äî Tenancy, Auth & Rate-Limit

Objetivo: Multi-tenant seguro, com API keys (HMAC) e/ou JWT, rate-limit e quotas.

Tarefas

 Auth HMAC

 Middleware HTTP/gRPC: valida X-Api-Key, X-Timestamp, X-Nonce, X-Signature.

 Prote√ß√£o de replay (timestamp ¬±5min, nonce em Redis/mem 10min).

 JWT (opcional)

 Valida√ß√£o JWKS, claims tenant_id, project_id, scopes:["infer"].

 Rate-limit

 Por {tenant_id,project_id}, RPS + burst (token bucket).

 Quotas (opcional)

 Contabilizar tokens/dia por tenant (se providers reportarem).

Arquivos

llm-pool/src/security/{hmac.rs,jwt.rs,ratelimit.rs,nonce_cache.rs}

Testes

 HMAC v√°lido/inv√°lido, replay detectado.

 429 ao estourar rate-limit.

 JWT expirado/sem escopo.

CI

 Security tests rodam isolados; lint de headers obrigat√≥rios.

Docs

 docs/SECURITY.md (como gerar chaves, rotacionar, exemplos de assinatura).

L5 ‚Äî Configura√ß√£o & Hot-Reload

Objetivo: Arquivo √∫nico (llm-pool.toml) governando providers, ensemble, QoS, tenants.

Tarefas

 Schema

 [server], [qos], [ensemble], [breaker], [cache], [tenancy.*], [[providers]], [judge].

 Hot-reload

 Watcher aplica mudan√ßas sem reiniciar; valida√ß√£o e rollback se inv√°lido.

 Defaults seguros

 max_deadline_ms, hedge_after_ms, fail_rate, window_size.

Arquivos

llm-pool/src/config.rs

configs/llm-pool.example.toml

Testes

 Reload preserva conex√µes em andamento.

 Rejeita config inv√°lida com log claro.

Docs

 docs/CONFIG.md (todos campos + exemplos).

L6 ‚Äî Cache & Idempot√™ncia

Objetivo: Reduzir lat√™ncia/custos com cache por {task,prompt,max_tokens} e idempot√™ncia por request_id.

Tarefas

 Cache

 Driver mem√≥ria/Redis; TTL por task; ‚Äúdo not cache‚Äù opcional por meta.

 Idempot√™ncia

 Mem√≥ria/Redis guarda Answer por request_id (curto prazo, p.ex. 10‚Äì15 min).

Arquivos

llm-pool/src/cache/{mod.rs,redis.rs,memory.rs}

Testes

 Cache hit ratio sobe em prompts repetidos.

 Idempotente: mesma resposta para request_id repetido.

Docs

 docs/CACHE.md (chave, TTLs, invalida√ß√£o).

L7 ‚Äî Observabilidade (m√©tricas, logs, tracing)

Objetivo: Visibilidade total multi-tenant.

Tarefas

 M√©tricas Prometheus

 llmpool_infer_duration_ms{tenant,project,task,strategy,provider}

 llmpool_timeouts_total{‚Ä¶}

 llmpool_ensemble_decision_total{strategy}

 llmpool_provider_win_rate{provider,task}

 llmpool_cache_hit_ratio

 llmpool_breaker_open_total

 Logs estruturados

 JSON com request_id, tenant, project, task, strategy, winner_model, duration_ms, from_cache.

 Tracing

 OpenTelemetry; propagar trace_id do cliente.

Arquivos

llm-pool/src/telemetry/{metrics.rs,logging.rs,tracing.rs}

grafana/dashboards/llm-pool.json

Testes

 Smoke: endpoints de /metrics carregam; painel renderiza.

Docs

 docs/OBSERVABILITY.md (pain√©is, KPIs, alertas recomendados).

L8 ‚Äî SDKs (Rust, Python, TypeScript)

Objetivo: Clientes leves e idiom√°ticos.

Tarefas

 Rust: llmpool_sdk

 Client::infer(Query) com retries leves e mapeamento gRPC/HTTP.

 Python: llmpool (httpx/grpc)

 Client.infer(task, strategy, prompt, ...) + HMAC helper.

 TypeScript: @vvtv/llmpool-client

 Fetch com HMAC/JWT; tipagens a partir do proto.

Arquivos

sdk/rust/llmpool_sdk/*

sdk/python/llmpool/*

sdk/ts/llmpool-client/*

Testes

 Cada SDK com server fake (contract tests).

 Exemplos prontos (planner/curator samples).

Docs

 docs/SDKs.md (uso, autentica√ß√£o, exemplos por task).

L9 ‚Äî Deploy & Gateway (exposi√ß√£o segura na internet)

Objetivo: Publicar o pool com chaves e WAF sem expor o backend cru.

Tarefas

 Container & Helm

 Dockerfile multi-stage; charts/llm-pool (replicas, HPA, liveness, readiness).

 Gateway/WAF

 Kong/Cloudflare/NGINX: TLS, rate-limit, valida√ß√£o de HMAC/JWT, mTLS gateway‚Üîpool.

 (Opcional) IP allowlist; limitar size de payload.

 Secrets & rota√ß√£o

 Duas chaves ativas por cliente (A/B); runbook de rota√ß√£o.

 SRE runbook

 Quedas de provider, breaker aberto, satura√ß√£o, emerg√™ncia (kill switch).

Arquivos

deploy/Dockerfile

deploy/charts/llm-pool/*

gateway/kong.yaml ou cloudflare/terraform/*

RUNBOOK_llmpool.md

Testes

 E2E p√∫blico (lat√™ncia p95, rate-limit, HMAC v√°lido/ inv√°lido).

 mTLS habilitado entre gateway‚Üîpool.

Docs

 docs/EXPOSURE.md (topologia, headers, exemplos de assinatura).

üìé Extras √∫teis (Qualidade de vida)

 Feature flag no cliente: backend = "local" | "pool".

 Kill-switch por task/tenant (desabilitar JUDGE ou RECOVERY_PLAN r√°pido).

 Policy de custos por tenant: max_tokens, max_deadline, max_parallel_providers.

üî¢ Ordem sugerida (com paralelismo)

L1 (Core) + L5 (Config) + L7 (b√°sico de m√©tricas)

L2 (Providers)

L3 (Ensemble & QoS)

L4 (Auth/Rate-limit)

L6 (Cache/Idempot√™ncia)

L8 (SDKs)

L9 (Deploy+Gateway)

‚úÖ Crit√©rios de aceite finais

p95 Infer (FASTEST, 2 providers, hedge 350ms) ‚â§ 900ms em rede local/gateway.

Timeout rate < 10% por task/tenant (breaker atuando).

Cache hit ‚â• 30% em prompts repetidos (workload real).

Determinismo em VOTING/CONSENSUS/JUDGE para a mesma entrada.

Seguran√ßa: HMAC/JWT verificados, rate-limit ativo, replay bloqueado.

Observabilidade: m√©tricas e logs com tenant/project/task/strategy/provider.

SDKs publicados e exemplos rodando (planner/curator/autopilot).

Se quiser, converto esse plano em issues do GitHub (com labels, depend√™ncias) e j√° te mando os scaffolds de pastas/arquivos para come√ßar a abrir PRs.

Voc√™ disse:
Qgora a lista de quais modelos usar e como, quando
O ChatGPT disse:
Pensou por 27s

Perfeito, Dan. Abaixo est√° a curadoria dos modelos para o seu LLM Pool ‚Äî quais usar, quando, como e por qu√™ ‚Äî j√° pensando no seu SLA (respostas <1,2s nos hooks online) e nos modos batch (auditoria/relat√≥rios).

üéõÔ∏è Matriz ‚ÄúTarefa ‚Üí Modelo ‚Üí Como usar‚Äù

Regras gerais
‚Ä¢ Hook on-line (‚â§1200 ms): priorize 8‚Äì14B quantizado (INT4/8) com hedged requests.
‚Ä¢ Judge/Rerank de alta precis√£o (at√© 2,5s): suba para 14‚Äì32B (ou MoE).
‚Ä¢ Batch noturno (sem pressa): modelos >30B/MoE.
‚Ä¢ Servir via vLLM (OpenAI-compatible) ou Ollama como fallback local. 
GitHub
+1

1) expand_queries (ampliar termos de busca)

Prim√°rio (r√°pido, barato): Phi-3 Medium 14B (ou 4‚Äì7B se GPU apertada). Excelente em s√≠ntese/expans√£o curta; roda liso quantizado. 
Ollama

Alternativa com mais contexto: Llama 3.1 8B (ou 70B em batch). Boa compreens√£o geral e janelas longas. 
Ollama
+1

Quando usar: todo ciclo de procura/planejamento; TTL de cache alta (mesma query se repete).

2) site_tactics (estrat√©gia de navega√ß√£o/scrape humano-simulada)

Prim√°rio (robusto a instru√ß√µes longas): Llama 3.1 8B/70B (on-line 8B; 70B para gerar ‚Äúplaybooks‚Äù em batch). 
Ollama
+1

Alternativa com racioc√≠nio t√°tico: Qwen3 14‚Äì32B (boa leitura de p√°ginas e instru√ß√µes multi-etapas; Apache 2.0 nas variantes abertas). Use em batch para gerar macro-estrat√©gias por dom√≠nio. 
Wikipedia

3) rerank_candidates (ordenar 10‚Äì12 candidatos)

Prim√°rio (on-line ‚â§1,2s): Gemma 2 9B/27B ‚Äî √≥timos ‚Äúrerankers‚Äù gerais; 9B para on-line, 27B para prime time (2¬™ passada). 
Ollama

‚ÄúTaste engine‚Äù em batch: Llama 3.1 70B ou Qwen3 32B para rerank com justificativa (logs do Curador). 
The Guardian
+1

4) judge (LLM-as-judge entre respostas)

Prim√°rio (curto e assertivo): Llama 3.1 8B/70B com prompt de julgamento enxuto; limite max_tokens ‚â§ 128. 
Ollama
+1

Juiz ‚Äúesperto‚Äù para racioc√≠nio: Qwen3 14‚Äì32B (modo batch em revis√µes di√°rias). 
Wikipedia

5) recovery_plan (quando algo falha em runtime)

Prim√°rio (resposta sucinta): Phi-3 Medium 14B (foca em plano de a√ß√£o curto). 
Ollama

Secund√°rio (cen√°rios complexos): Gemma 2 27B (mais ‚Äúcalmo‚Äù em instru√ß√µes estruturadas). 
Ollama

6) enrich_metadata (t√≠tulos/tags/resumos)

Prim√°rio (lat√™ncia baixa): Phi-3 Mini/Medium para tagging r√°pido. 
Ollama

Alternativa criativa controlada: Llama 3.1 8B (ou 70B em lote para copy premium do dia). 
Ollama
+1

Observa√ß√£o sobre Qwen (fam√≠lia 2.5/3): vers√µes Qwen3 8/14/32B s√£o fortes em racioc√≠nio e dispon√≠veis sob Apache 2.0 para self-hosting, boas para judge, rerank e site tactics em batch. 
Wikipedia

‚öôÔ∏è Perfis do Pool (o ‚Äúcard√°pio‚Äù)
Perfil	Estrat√©gia	Modelos (ordem de hedge)	Uso
FAST-LOW	FASTEST + hedge 300 ms	Phi-3 14B ‚Üí Llama 3.1 8B	hooks on-line (expand/enrich)
JUDGE-MID	JUDGE ‚Üí VOTING	Llama 3.1 8B (judge) + Gemma 2 9B (candidatos)	rerank e judge on-line
QUALITY-HIGH	CONSENSUS	Llama 3.1 70B + Qwen3 32B	rerank/judge em batch (curadoria)
COST-SAVER	WEIGHTED	Phi-3 14B (peso 0.6) + Gemma 2 9B (0.4)	tarefas massivas (tags)
ROBUST	VOTING ‚Üí JUDGE	Gemma 2 27B + Llama 3.1 8B + Phi-3 14B	quando vari√¢ncia de resposta √© alta

Servidores: preferir vLLM (alto throughput, batching cont√≠nuo, quantiza√ß√µes INT4/8, API OpenAI-like). Manter Ollama como dev/local fallback e prototipagem. 
GitHub
+1

üñ•Ô∏è Hardware & Deployment

N√≥s on-line (baixa lat√™ncia):

1√ó A4000/RTX 6000 Ada 48‚Äì96 GB ‚Üí Llama 3.1 8B / Gemma 2 9B / Phi-3 14B INT4/8.

2√ó GPUs + vLLM com continuous batching para p95 est√°vel. 
GitHub

N√≥s batch (noite):

1√ó A100 80GB / H100 para Llama 3.1 70B / Qwen3 32B e Mixtral-class (se adotado).

Quantiza√ß√£o: INT4/8 (AWQ/GPTQ/FP8 onde suportado no vLLM). 
GitHub

üß≠ Roteamento (quando escolher cada um)

Entrada curta (<1k tokens), prazo ‚â§1200 ms:
‚Üí FAST-LOW (Phi-3 14B ‚Üí hedge Llama 3.1 8B).

Disputa entre 3‚Äì5 candidatos:
‚Üí JUDGE-MID (Llama 3.1 8B como juiz).

Especial ‚Äúprime time‚Äù/cole√ß√µes do dia (offline):
‚Üí QUALITY-HIGH (Llama 3.1 70B + Qwen3 32B consenso).

Milhares de metadados:
‚Üí COST-SAVER (Phi-3 14B + Gemma 2 9B, pesados no cache).

Quando um provedor degrada (timeouts >10%):
‚Üí Circuit breaker abre; cair para Ollama local do mesmo tamanho (ou provider alternativo via vLLM). 
GitHub

üß™ Prompting e limites pr√°ticos

JUDGE: prompt fixo, few-shot 1‚Äì2 linhas, max_tokens ‚â§128 (corta lat√™ncia).

RERANK: passe apenas features essenciais (dura√ß√£o, tags, resumo 1‚Äì2 frases).

EXPAND: delimite output schema (lista JSON de termos + pesos).

RECOVERY: pe√ßa at√© 3 passos com owner checks (evita del√≠rio).

ENRICH: templates com campos curtos (titulo ‚â§60 chars, 4‚Äì8 tags).

üß∞ Licen√ßas & praticidades

Llama 3.1 (Meta) ‚Äî open p/ uso local; tamanhos 8B‚Äì405B dispon√≠veis via Ollama/vLLM. 
Ollama
+1

Gemma 2/3 (Google) ‚Äî boa para 1 GPU; aten√ß√£o a licen√ßa (restringe alguns usos comerciais; verifique clausulado). 
theverge.com

Phi-3 (Microsoft) ‚Äî small/medium muito eficientes. 
Ollama

Qwen3 (Alibaba/Qwen) ‚Äî ampla fam√≠lia Apache 2.0 em 2025 (8B‚Äì32B); forte em racioc√≠nio. 
Wikipedia

Observa√ß√£o: modelos e licen√ßas evoluem r√°pido; manter invent√°rio mensal e bench interno em seu tr√°fego (lat√™ncia, win-rate por tarefa).

üß© Exemplo de pool profile (llm-pool.toml)
[qos]
max_deadline_ms = 1500
hedge_after_ms  = 300

[ensemble]
default = "FASTEST"

[[providers]]
name = "vllm-phi3-14b"
type = "vllm"
model = "microsoft/phi-3-medium-128k-instruct"
tasks = ["expand_queries","enrich_metadata","recovery_plan"]

[[providers]]
name = "vllm-llama31-8b"
type = "vllm"
model = "meta-llama/llama-3.1-8b-instruct"
tasks = ["judge","rerank_candidates","site_tactics","expand_queries"]

[[providers]]
name = "vllm-gemma2-9b"
type = "vllm"
model = "google/gemma-2-9b-it"
tasks = ["rerank_candidates","enrich_metadata"]

[[providers]]
name = "vllm-qwen3-32b"
type = "vllm"
model = "qwen/qwen3-32b-instruct"
tasks = ["judge","rerank_candidates"]         # batch

[judge]
strategy = "JUDGE"
fallback = "VOTING"

TL;DR (escolhas r√°pidas)

On-line: Phi-3 14B + Llama 3.1 8B (r√°pidos); Gemma 2 9B para rerank. 
Ollama

Batch qualidade: Llama 3.1 70B + Qwen3 32B (consenso/judge). 
The Guardian
+1

Infra: servir por vLLM (OpenAI-like API, batching, quantiza√ß√µes); Ollama para dev e fallback. 
GitHub
+1

Se quiser, j√° te entrego os arquivos de config iniciais com esses perfis e um script de bench interno (lat√™ncia/win-rate por tarefa) pra validar no seu tr√°fego real.

Voc√™ disse:
Decide pra mim o que fazerm so lembrando que tem que rodar no macmini e nao em 2 gpu
O ChatGPT disse:
Pensou por 5s

Perfeito ‚Äî decis√£o fechada para rodar 100% no Mac mini (Apple Silicon, 16‚Äì32 GB RAM, sem GPU NVIDIA) usando Ollama (Metal) como runtime. Foquei em lat√™ncia <1,2 s nos hooks on-line, footprint de mem√≥ria razo√°vel e qualidade suficiente para judge/rerank.

‚úÖ Escolha final (pronta pra usar)
Pool on-line (tempo-real)

FAST: Phi-3 Mini 4B (Instruct) ‚Äî super leve e r√°pido; √≥timo p/ expandir buscas, enriquecer metadados e planos de recupera√ß√£o curtinhos.

GENERAL: Llama 3.1 8B Instruct ‚Äî ‚Äúc√©rebro geral‚Äù do pool, bom equil√≠brio em quase tudo; ideal p/ judge curto e site tactics sucintas.

RERANK/TASTE: Gemma 2 9B It (ou Qwen 2.5/3 7B Instruct se preferir Apache 2.0) ‚Äî melhora rerank e refinamento editorial com pouco custo extra.

Por que esses?
‚Ä¢ Todos rodam bem no Mac mini via Metal/llama.cpp (Ollama) com quantiza√ß√£o Q4_K_M.
‚Ä¢ 4B + 8B + ~9B d√° diversidade pro ensemble sem matar RAM.

Estrat√©gias por tarefa
Tarefa (hook)	Estrat√©gia	Modelos	Observa√ß√µes
expand_queries	FASTEST + hedge 300 ms	Phi-3 4B ‚Üí Llama 3.1 8B	cache alto (TTL 10‚Äì30 min)
site_tactics	JUDGE ‚Üí VOTING	Llama 3.1 8B (juiz) + Gemma 2 9B	prompts curtos, max_tokens ‚â§128
rerank_candidates	JUDGE	Llama 3.1 8B (juiz)	passe s√≥ features essenciais
recovery_plan	FASTEST	Phi-3 4B ‚Üí Llama 3.1 8B	3 passos no m√°x.
enrich_metadata	FASTEST	Phi-3 4B	t√≠tulos ‚â§60 chars, 4‚Äì8 tags

Batch noturno (opcional, mesmo Mac mini): usar Llama 3.1 8B e Gemma 2 9B com deadline maior (2‚Äì3 s) para gerar memos/editoriais. N√£o recomendo modelos >10‚Äì13B no Mac mini ‚Äî custo/lat√™ncia sobem muito.

üì¶ Instala√ß√£o (Ollama)
# 1) Instalar Ollama (macOS)
brew install ollama
ollama serve &

# 2) Baixar modelos (quantiza√ß√£o default do Ollama j√° √© √≥tima no Mac)
ollama pull phi3:mini        # microsoft/phi-3-mini
ollama pull llama3.1:8b      # meta-llama/llama-3.1-8b-instruct
ollama pull gemma2:9b        # google/gemma-2-9b-it
# (opcional) alternativa Apache 2.0:
# ollama pull qwen2.5:7b-instruct


Dica RAM: se seu Mac mini for 16 GB, rode 2 modelos ativos (Phi-3 4B + Llama 3.1 8B) e carregue o 3¬∫ on-demand (Gemma/Qwen) s√≥ para rerank/judge.

‚öôÔ∏è llm-pool.toml (pronto pro seu servi√ßo)
[server]
listen_addr = "0.0.0.0:7070"
enable_grpc = true
enable_http = true

[qos]
max_deadline_ms = 1500
hedge_after_ms  = 300
max_prompt_bytes = 16384
max_tokens_default = 256

[ensemble]
default_strategy = "FASTEST"
strategy_by_task.expand_queries   = "FASTEST"
strategy_by_task.site_tactics     = "JUDGE"
strategy_by_task.rerank_candidates= "JUDGE"
strategy_by_task.recovery_plan    = "FASTEST"
strategy_by_task.enrich_metadata  = "FASTEST"

[breaker]
fail_rate   = 0.10
window_size = 50
open_cooldown_ms = 300000

[cache]
enabled = true
driver  = "memory"
ttl_seconds = 900
key_fields = ["task","prompt","max_tokens"]

[[providers]]
name   = "ollama-phi3-mini"
driver = "ollama"
base_url = "http://127.0.0.1:11434"
model  = "phi3:mini"
tasks  = ["expand_queries","enrich_metadata","recovery_plan"]
weight = 0.6

[[providers]]
name   = "ollama-llama31-8b"
driver = "ollama"
base_url = "http://127.0.0.1:11434"
model  = "llama3.1:8b"
tasks  = ["judge","site_tactics","rerank_candidates","expand_queries","recovery_plan"]
weight = 1.0

[[providers]]
name   = "ollama-gemma2-9b"
driver = "ollama"
base_url = "http://127.0.0.1:11434"
model  = "gemma2:9b"
tasks  = ["rerank_candidates","judge"]
weight = 0.8

[judge]
model_provider = "ollama-llama31-8b"
max_tokens = 128
deadline_ms = 700
fallback_strategy = "VOTING"

üîß Prompts & limites (para segurar p95)

JUDGE/RE-RANK: prompt fixo ultra curto + schema de sa√≠da (JSON com winner_idx, why), max_tokens ‚â§128.

EXPAND: pe√ßa lista JSON de termos + pesos (no m√°x. 16 termos).

RECOVERY: ‚Äúdiga at√© 3 passos‚Äù, sempre com guardrails.

ENRICH: templates (t√≠tulo ‚â§60, 4‚Äì8 tags).

üß™ Sanity check (targets no Mac mini)

expand/enrich: 200‚Äì700 ms (Phi-3 4B)

judge/rerank simples: 700‚Äì1200 ms (Llama 3.1 8B)

Batch noturno (memos/editoriais): 1,5‚Äì3,0 s (Llama 8B / Gemma 9B)

TL;DR

Use Phi-3 4B (r√°pido), Llama 3.1 8B (juiz geral) e Gemma 2 9B (rerank fino). Tudo via Ollama no Mac mini, com estrat√©gias FASTEST/JUDGE, prompts curtos e cache. Se preferir licen√ßa Apache full, troque Gemma 2 9B por Qwen 2.5/3 7B.

Se quiser, te mando tamb√©m os prompts prontos (JUDGE/RERANK/EXPAND/RECOVERY/ENRICH) em .md pra colar no reposit√≥rio.

Voc√™ disse:
Manda
O ChatGPT disse:

maravilha ‚Äî seguem os 5 prompts prontos (em arquivos .md) para colar no seu reposit√≥rio (/prompts). Est√£o otimizados para rodar no Mac mini com Ollama: curtos, determin√≠sticos, com schemas JSON simples (f√°ceis de parsear) e guardrails fortes.

Dica: mantenha max_tokens em 128 (JUDGE/RERANK) e 256 (EXPAND/RECOVERY/ENRICH).
Todos os prompts exigem sa√≠da estritamente em JSON (sem texto extra).

/prompts/judge.md
# JUDGE ‚Äî sele√ß√£o determin√≠stica do melhor candidato

Voc√™ √© um **√°rbitro conciso**. Recebe 3‚Äì12 candidatos e retorna **apenas 1 vencedor** com justificativa breve.

## Regras
- Siga o **crit√©rio** informado em `decision_criterion`.
- **Nunca** invente fatos, n√∫meros, t√≠tulos ou URLs.
- **Recuse** candidatos que aparentem violar pol√≠ticas (ex.: conte√∫do que sugira menores, coer√ß√£o, ilegalidade). Se todos violarem, retorne `"winner_id": null` com `"reason"`.
- Priorize **qualidade visual** (‚â•720p), **dura√ß√£o-alvo** (ex.: ~10min), **variedade est√©tica** e **coer√™ncia** com o slot.
- Sa√≠da **somente JSON**, sem texto adicional.

## Entrada (JSON)
```json
{
  "decision_criterion": "choose best for prime-time balance of novelty + duration ~10min + visual quality",
  "slot_context": {"slot_id": "86", "recent_themes": ["studio","warm-pastel"], "target_duration_sec": 600},
  "candidates": [
    {
      "id":"c1",
      "title":"‚Ä¶",
      "tags":["‚Ä¶"],
      "duration_sec": 604,
      "resolution":"1080p",
      "novelty_score":0.61,
      "palette":["warm","pastel"],
      "qc":{"vmaf":91,"lufs":-14.2},
      "policy_flags":{"suspected_minor":false,"violence":false,"drm_locked":false}
    }
    // ‚Ä¶ c2..cN
  ]
}

Sa√≠da (JSON)
{
  "winner_id": "c1",
  "confidence": 0.74,
  "reason": "1080p, ~10min, contrasta temas recentes; melhor vmaf; variedade est√©tica.",
  "quality_notes": ["bom √°udio (-14 LUFS)", "paleta distinta do hist√≥rico"],
  "policy": {"action": "allow", "notes": []}
}

Diretrizes de avalia√ß√£o (resumo)

Ader√™ncia ao crit√©rio do slot; 2) Qualidade t√©cnica (resolu√ß√£o/VMAF/LUFS); 3) Variedade vs. √∫ltimos slots; 4) Dura√ß√£o pr√≥xima da meta; 5) Tags coerentes; 6) Sinaliza√ß√µes de pol√≠tica (reprovar se suspeito).


---

## `/prompts/rerank.md`

```md
# RERANK ‚Äî reordenar 10‚Äì12 candidatos para o slot

Voc√™ recebe uma lista de candidatos e deve retornar **apenas a ordem final** e um racional curto.

## Regras
- Otimize para **ritmo** (evitar repeti√ß√£o), **dura√ß√£o** (~10min), **qualidade t√©cnica** e **variedade de paleta/tema**.
- **N√£o remova** itens; **reordene**. Se algum for invi√°vel, marque em `policy.flags`.
- Sa√≠da **somente JSON**.

## Entrada (JSON)
```json
{
  "slot_context": {
    "slot_id":"86",
    "recent_slots":[
      {"id":"r11","palette":["warm","pastel"],"tags":["studio"],"duration_sec":602},
      {"id":"r12","palette":["warm"],"tags":["studio","closeup"],"duration_sec":593}
    ],
    "target_duration_sec": 600
  },
  "candidates":[
    {"id":"c1","duration_sec":604,"resolution":"1080p","palette":["cool"],"tags":["cinematic"],"qc":{"vmaf":90}},
    {"id":"c2","duration_sec":720,"resolution":"720p","palette":["warm"],"tags":["studio"],"qc":{"vmaf":87}}
    // ‚Ä¶ c3..c12
  ]
}

Sa√≠da (JSON)
{
  "ranking": ["c1","c5","c3","c2","c7","c8","c6","c11","c10","c4","c9","c12"],
  "diversity_gain": 0.07,
  "reasons": [
    "abre com paleta fria p/ contrastar hist√≥rico quente",
    "dur. pr√≥ximas de 10min nas 3 primeiras posi√ß√µes",
    "evita 2 'studio warm' consecutivos"
  ],
  "policy": {
    "flags": [],
    "notes": []
  }
}

Heur√≠stica (aplique em sil√™ncio)

Penalize sequ√™ncias com mesma paleta dos √∫ltimos 2‚Äì3 slots.

Bonifique 1080p e VMAF ‚â• 90.

Prefira 600¬±45s; durations muito fora v√£o para baixo.


---

## `/prompts/expand_queries.md`

```md
# EXPAND_QUERIES ‚Äî ampliar termos de busca (v√≠deos e m√∫sicas)

Gere **listas curtas e objetivas** para busca web. Foco em **diversidade controlada** e **baixo custo**.

## Regras
- N√£o use termos expl√≠citos; prefira **descri√ß√µes neutras/est√©ticas**.
- Inclua **negativos** para evitar duplicatas, baixa qualidade e termos proibidos.
- Sugira **sites gen√©ricos** (quando aplic√°vel), nunca invente URLs.
- Sa√≠da **somente JSON**.

## Entrada (JSON)
```json
{
  "seed_terms": ["cinematic lounge", "ambient slow burn"],
  "media": "video|music",
  "lang": ["pt","en"],
  "duration_target_sec": 600,
  "negatives_hint": ["cam rip","compilation lowres","duplicate"],
  "constraints": {"min_resolution":"720p","license_hint":"non-drm"}
}

Sa√≠da (JSON)
{
  "must_include": ["cinematic lounge", "ambient slow burn"],
  "optional": ["downtempo", "moody color grading", "studio soft light", "bokeh"],
  "negatives": ["cam rip","compilation","blurry","watermark","reaction","shorts"],
  "duration_range_sec": [540, 720],
  "site_hints": ["site:archive.org", "site:soundcloud.com", "site:bandcamp.com", "site:vimeo.com"],
  "language_mix": {"pt": 0.4, "en": 0.6},
  "notes": ["priorize '1080p' nas queries de v√≠deo", "adicionar -shorts -tiktok"]
}

Observa√ß√£o

Reutilize resultados com TTL de 10‚Äì30 min no cache do pool.


---

## `/prompts/recovery_plan.md`

```md
# RECOVERY_PLAN ‚Äî plano de a√ß√£o em at√© 3 passos

Quando algo falhar (ex.: baixa qualidade t√©cnica, buffer cr√≠tico, fonte sem HD), descreva **um plano sucinto e seguro**.

## Regras
- M√°ximo **3 passos**. Sem gambiarras, sem burlar ToS, sem instru√ß√µes invasivas.
- Se o cen√°rio indicar DRM/ABR indispon√≠vel, **recomende abortar** com raz√£o.
- Sa√≠da **somente JSON**.

## Entrada (JSON)
```json
{
  "incident": "low_vmaf_playout",
  "context": {
    "slot_id":"86",
    "current_asset":{"id":"a17","vmaf":81,"resolution":"720p","bitrate_kbps":1200},
    "buffer_minutes": 0.7,
    "alternatives":[{"id":"b02","vmaf":92,"resolution":"1080p"}]
  }
}

Sa√≠da (JSON)
{
  "severity": "medium",
  "steps": [
    {"action":"swap_asset","args":{"use_id":"b02"},"guardrails":["verify vmaf>=90","duration within 540..720"]},
    {"action":"bump_music_ratio","args":{"next_slots":2,"mood":"downtempo"},"guardrails":["keep queue FIFO within bucket"]},
    {"action":"note_incident","args":{"tag":"slow-HD-source"},"guardrails":["do not blacklist domain without audit"]}
  ],
  "rollback": {"if":"swap fails","do":"revert_to a17 and inject emergency loop 1 item"},
  "eta_min": 3,
  "reason": "asset b02 atende 1080p/VMAF92; troca r√°pida reduz risco de churn"
}

Notas

Use prefer√™ncia por alternativas j√° baixadas.

Se tudo abaixo do limiar, recomende emergency loop tempor√°rio.


---

## `/prompts/enrich_metadata.md`

```md
# ENRICH_METADATA ‚Äî t√≠tulo, tags e resumo curto

Crie metadados **claros e neutros**, focados em est√©tica/ritmo (sem termos expl√≠citos).

## Regras
- T√≠tulo ‚â§ **60** caracteres; resumo ‚â§ **120** caracteres.
- 4‚Äì8 **tags** objetivas (cores, clima, estilo).
- Se suspeitar de viola√ß√£o de pol√≠tica, sinalize em `content_flags`.
- Sa√≠da **somente JSON**.

## Entrada (JSON)
```json
{
  "asset": {
    "id":"c1",
    "features": {"palette":["cool","night"],"style":["cinematic","slow burn"],"duration_sec": 604, "resolution":"1080p"},
    "audio":{"lufs": -14.1},
    "tags":["studio","bokeh"]
  }
}

Sa√≠da (JSON)
{
  "title": "Cinematic Night ‚Äî slow burn em luz fria",
  "one_liner": "Texturas frias e ritmo suave com bokeh discreto e acabamento 1080p.",
  "tags": ["cinematic","cool","night","slow-burn","bokeh","1080p","studio"],
  "mood": "downtempo",
  "content_flags": [],
  "notes": ["mant√©m coer√™ncia com dura√ß√£o alvo ~10min"]
}

Observa√ß√µes

Evite palavras sensacionalistas; privilegie est√©tica e t√©cnica.


---

### Como usar (resumo r√°pido)

- **JUDGE/RERANK**: chame com `max_tokens=128`, `temperature‚âà0.2`, **deadline 700‚Äì1200ms**.  
- **EXPAND/RECOVERY/ENRICH**: `max_tokens=256`, `temperature‚âà0.3`, **deadline 800‚Äì1500ms**.  
- Sempre force **sa√≠da JSON** ‚Äî o pool j√° valida e rejeita fora do schema.

Se quiser, eu tamb√©m te mando uma **vers√£o .zip** com esses arquivos j√° nomeados e uma **pasta `examples/`** com entradas/sa√≠das de teste (posso gerar aqui mesmo).
::contentReference[oaicite:0]{index=0}