# ðŸš€ START HERE - LLM Pool Quick Setup

Welcome! This is your **LLM Pool service** - a smart orchestration layer for multiple LLM models running on your Mac mini.

## ðŸ“‹ What You Have

A complete, production-ready service with:
- âœ… **3 LLM models**: Phi-3 (4B), Llama 3.1 (8B), Gemma 2 (9B)
- âœ… **Dual APIs**: gRPC + HTTP/REST
- âœ… **Smart routing**: Tasks automatically use the best model
- âœ… **Ensemble strategies**: FASTEST, VOTING, JUDGE
- âœ… **Caching**: Instant responses for repeated queries
- âœ… **Hot-reload config**: Change settings without restart

## âš¡ Quick Start (3 Steps)

### Step 1: Download Models (15-20 min)

One model is already downloaded. Get the other two:

```bash
./setup-models.sh
```

**Or manually:**
```bash
ollama pull llama3.1:8b    # ~4.7GB
ollama pull gemma2:9b      # ~5.4GB
```

### Step 2: Build & Run (5 min)

```bash
# Make sure Ollama is running
ollama serve &

# Build and start the service
cargo run --release
```

Wait for this output:
```
âœ… gRPC server listening on 0.0.0.0:7070
âœ… HTTP server listening on 0.0.0.0:7071
```

### Step 3: Test It

Open a new terminal:

```bash
# Health check
curl http://localhost:7071/health | jq .

# Your first AI request
curl -X POST http://localhost:7071/v1/infer \
  -H "Content-Type: application/json" \
  -d '{
    "task": "expand_queries",
    "prompt": "Generate search terms for ambient videos"
  }' | jq .
```

**First request takes 20-30s** (model loading). After that: <1 second! âš¡

## ðŸ“š Documentation

Choose your path:

### ðŸŽ¯ **I want to get it running NOW**
â†’ Follow [CHECKLIST.md](CHECKLIST.md) - Step-by-step with checkboxes

### ðŸ“– **I want to understand what I'm building**
â†’ Read [SETUP_SUMMARY.md](SETUP_SUMMARY.md) - What was created and why

### ðŸ—ï¸ **I want to understand the architecture**
â†’ See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) - System design and flow

### ðŸ“˜ **I want the full documentation**
â†’ Read [README.md](README.md) - Complete guide with all features

### ðŸŽ“ **I want to learn step-by-step**
â†’ Follow [QUICKSTART.md](QUICKSTART.md) - Detailed tutorial

## ðŸŽ¯ What Can It Do?

### Task Examples

**1. Expand Search Queries** (Fast - Phi-3)
```bash
curl -X POST http://localhost:7071/v1/infer \
  -H "Content-Type: application/json" \
  -d '{
    "task": "expand_queries",
    "prompt": "cinematic lounge ambient",
    "max_tokens": 256
  }'
```

**2. Judge Best Option** (Smart - Llama 3.1)
```bash
curl -X POST http://localhost:7071/v1/infer \
  -H "Content-Type: application/json" \
  -d '{
    "task": "judge",
    "prompt": "Select best: A) 720p 8min studio, B) 1080p 10min cinematic",
    "strategy": "JUDGE"
  }'
```

**3. Enrich Metadata** (Fast - Phi-3)
```bash
curl -X POST http://localhost:7071/v1/infer \
  -H "Content-Type: application/json" \
  -d '{
    "task": "enrich_metadata",
    "prompt": "Create title and tags for a cinematic video with bokeh"
  }'
```

## ðŸŽ¨ Available Tasks

| Task | Speed | Model | Use Case |
|------|-------|-------|----------|
| `expand_queries` | âš¡ Fast | Phi-3 | Generate search terms |
| `enrich_metadata` | âš¡ Fast | Phi-3 | Create titles/tags |
| `recovery_plan` | âš¡ Fast | Phi-3 | Error recovery steps |
| `judge` | ðŸŽ¯ Smart | Llama | Select best option |
| `rerank_candidates` | ðŸŽ¯ Smart | Llama + Gemma | Reorder results |
| `site_tactics` | ðŸŽ¯ Smart | Llama + Gemma | Navigation strategy |

## ðŸ”§ Configuration

Edit `llm-pool.toml` to customize:

```toml
[ensemble]
default_strategy = "FASTEST"  # or "VOTING", "JUDGE"

[qos]
max_deadline_ms = 1500        # Max request time
hedge_after_ms = 300          # When to fire backup

[cache]
enabled = true
ttl_seconds = 900             # 15 minutes
```

**The service auto-reloads!** No restart needed. ðŸ”„

## ðŸ“Š Performance Expectations

| Metric | Value | Notes |
|--------|-------|-------|
| First request | 20-40s | Model loading |
| Fast tasks (Phi-3) | 200-700ms | expand, enrich |
| Smart tasks (Llama) | 700-1200ms | judge, rerank |
| Cache hits | <10ms | Instant! |
| Memory usage | 8-12GB | With 2-3 models |

## ðŸ†˜ Troubleshooting

### "Connection refused"
```bash
# Start Ollama
ollama serve &
```

### "Model not found"
```bash
# Download missing model
ollama pull phi3:mini
ollama pull llama3.1:8b
ollama pull gemma2:9b
```

### First request times out
**This is normal!** Models take 20-30s to load. Try again - next request will be fast.

### Out of memory
- Close other apps
- Use only 2 models (comment out one in config)

## ðŸ“ Project Structure

```
llm-pool/
â”œâ”€â”€ START_HERE.md          â† You are here!
â”œâ”€â”€ CHECKLIST.md           â† Step-by-step setup
â”œâ”€â”€ QUICKSTART.md          â† Detailed tutorial
â”œâ”€â”€ README.md              â† Full documentation
â”œâ”€â”€ SETUP_SUMMARY.md       â† What was built
â”‚
â”œâ”€â”€ llm-pool.toml          â† Configuration (edit this!)
â”œâ”€â”€ prompts/               â† Task templates
â”‚   â”œâ”€â”€ judge.md
â”‚   â”œâ”€â”€ rerank.md
â”‚   â”œâ”€â”€ expand_queries.md
â”‚   â”œâ”€â”€ recovery_plan.md
â”‚   â””â”€â”€ enrich_metadata.md
â”‚
â”œâ”€â”€ src/                   â† Rust source code
â”‚   â”œâ”€â”€ main.rs
â”‚   â”œâ”€â”€ orchestrator.rs
â”‚   â”œâ”€â”€ ensemble.rs
â”‚   â”œâ”€â”€ providers/
â”‚   â””â”€â”€ server/
â”‚
â”œâ”€â”€ setup-models.sh        â† Download all models
â””â”€â”€ test-http.sh           â† Test suite
```

## ðŸŽ¯ Next Steps

1. âœ… **Get it running** - Follow Step 1-3 above
2. ðŸ“– **Read the docs** - Pick from the list above
3. ðŸ§ª **Try examples** - Run `./test-http.sh`
4. ðŸŽ¨ **Explore tasks** - Try different task types
5. âš™ï¸ **Tune config** - Edit `llm-pool.toml`
6. ðŸš€ **Build something** - Integrate into your app!

## ðŸ’¡ Pro Tips

- **Cache is your friend**: Repeated prompts are instant
- **Start with FASTEST**: It's the quickest strategy
- **Use JUDGE for quality**: When you need the best answer
- **Watch the logs**: See which models are selected
- **Hot-reload config**: Edit and save, no restart needed

## ðŸŽ‰ Success Checklist

You're ready when you see:

- âœ… Service starts without errors
- âœ… Health check shows all providers healthy
- âœ… First request completes (even if slow)
- âœ… Second request is fast (<1s)
- âœ… Cache works (repeat = instant)

## ðŸ“ž Need Help?

1. Check [CHECKLIST.md](CHECKLIST.md) - Common issues
2. Read [QUICKSTART.md](QUICKSTART.md) - Detailed guide
3. Review logs - They're very descriptive
4. Check Ollama: `curl http://localhost:11434/api/tags`

---

## ðŸš€ Ready? Let's Go!

```bash
# 1. Download models
./setup-models.sh

# 2. Start service
cargo run --release

# 3. Test it (in new terminal)
curl http://localhost:7071/health | jq .
```

**Welcome to LLM Pool!** ðŸŽ‰

For the full experience, continue to [CHECKLIST.md](CHECKLIST.md) â†’
